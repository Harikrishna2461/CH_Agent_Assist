{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "f2601a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary_data : lambda_handler--output_from_the_llm_model : \n",
      "    Summary:\n",
      "    The customer, Michael Gonzalez, called to inquire about enrolling in the patient service program. The agent confirmed the customer's details such as gender, date of birth, address, and phone number. The customer provided their insurance information and preferred pharmacy. The agent then sent a consent form to the customer via text message, which they submitted and completed the enrollment process. The agent confirmed that the enrollment process is complete, and the customer will receive an email from the team in the next 48 hours.\n",
      "    \n",
      "    Next Steps:\n",
      "    • The team will reach out to the customer via email in the next 48 hours.\n",
      "    • The customer will receive a text message with the consent form.\n",
      "    • The customer completed the enrollment process..\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import base64\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import logging\n",
    "import redis\n",
    "import os\n",
    "from datetime import datetime\n",
    "from botocore.config import Config\n",
    "from botocore.exceptions import ClientError\n",
    "config = Config(read_timeout=1000)\n",
    "\n",
    "# Bedrock Runtime client initialization\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name=\"bedrock-runtime\",\n",
    "    region_name= os.environ.get(\"Region\"), #\"us-east-1\",\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Bedrock client initialization\n",
    "bedrock = boto3.client(\n",
    "    service_name='bedrock',\n",
    "    region_name= os.environ.get(\"Region\") #'us-east-1'\n",
    ")\n",
    "# Initialize Redis client\n",
    "redis_host = 'ch-agent-assist-redis-cluster-disabled.tjyhst.ng.0001.use1.cache.amazonaws.com'\n",
    "redis_port = 6379  # Default Redis port is 6379\n",
    "redis_password = None  # None if no password\n",
    "redis_client = redis.Redis(host=redis_host, port=redis_port, password=redis_password, decode_responses=True)\n",
    "\n",
    "##Processing data to fetch role and content\n",
    "def construct_call_conversation(data):\n",
    "    segments = json.loads(data)\n",
    "    convo = \"\" \n",
    "    # Extract transcripts, participant roles, and content\n",
    "    for segment in segments:\n",
    "        transcript = segment['transcript'][0]\n",
    "        participant_role = transcript['ParticipantRole']\n",
    "        content = transcript['Content']\n",
    "        convo += participant_role + \" : \" + content + \"\\n\"\n",
    "        \n",
    "    return convo\n",
    "\n",
    "def model_output_postprocessing(data):\n",
    "    result = \"\"\n",
    "    start_index = data.find(\"{\")\n",
    "    \n",
    "    if start_index == -1:\n",
    "        data1 = \"{\" + data\n",
    "    else:\n",
    "        data1 = data\n",
    "    \n",
    "    start_index_final = data.find(\"{\")    \n",
    "    print(f'start_index:{start_index_final}')\n",
    "    \n",
    "    end_char_indices = [i.start() for i in re.finditer(\"}\",data1)]\n",
    "    print(f'end_indices:{end_char_indices}')\n",
    "    end_index = end_char_indices[len(end_char_indices)-1]\n",
    "    print(f'end_index:{end_index}')\n",
    "    if end_index == len(data1)-1:\n",
    "        result = data1[start_index_final:]\n",
    "    else:\n",
    "        result = data1[start_index_final:end_index+1]\n",
    "    print(f'result:{result}')\n",
    "    \n",
    "    return result\n",
    "    \n",
    "def summarisation_prompt_generator(context):\n",
    "    prompt_llama = f\"\"\"\n",
    "Instruction: \"Summarise this call transcript between a patient and an agent and include the information shared by the patient in a precise paragraph\":\n",
    "\n",
    "Addtionally, use bullet points(*) and highlight the next steps mentioned in the transcript under a separate heading.\n",
    "\n",
    "Do not give redundant information.\n",
    "Do not create any headings for Summary.\n",
    "\n",
    "NOTE: Consider the below context as your only source of information and do not create any additional information or context\n",
    "\n",
    "{context}\n",
    "             \n",
    "             \n",
    "Response :\n",
    "\n",
    "    \"\"\"        \n",
    "    return prompt_llama\n",
    "\n",
    "def get_summary_prompt(bucket,file,prompt_category,required_prompt,conversation):\n",
    "    s3 = boto3.client('s3') \n",
    "    response = s3.get_object(Bucket=bucket,Key=file)\n",
    "    content = response['Body'].read().decode('utf-8')\n",
    "    json_content = json.loads(content)\n",
    "    prompt = json_content[prompt_category][required_prompt].format(context=conversation)\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "#Defining function to summarize context\n",
    "def load_llama2(bedrock_runtime , prompt , temp , top_p):\n",
    "    try:\n",
    "        body = {\n",
    "            \"prompt\" : prompt,\n",
    "            \"temperature\" : temp,\n",
    "            \"top_p\" : top_p,\n",
    "            \"max_gen_len\" : 1000\n",
    "            }\n",
    "\n",
    "        response = bedrock_runtime.invoke_model(\n",
    "            modelId=\"meta.llama2-13b-chat-v1\", body=json.dumps(body)\n",
    "        )\n",
    "\n",
    "        response_body = json.loads(response[\"body\"].read())\n",
    "        completion = response_body[\"generation\"]\n",
    "\n",
    "        return completion\n",
    "\n",
    "    except ClientError:\n",
    "        logging.error(\"Couldn't invoke Llama 2\")\n",
    "        raise  \n",
    "\n",
    "data ='''\n",
    "\"AGENT\" : \"Welcome to patient bad with this is there on March? Who do I have the pleasure of speaking with today?\"\n",
    "\n",
    "\"CUSTOMER\" : \"Hi. This is Michael Gonzalez.\"\n",
    "\n",
    "\"AGENT\" : \"Same here. How can I help you today?\"\n",
    "\n",
    "\"CUSTOMER\" : \"I wanted to inquire about enrolling in the patient service program.\"\n",
    "\n",
    "\"AGENT\" : \"Great. Uh, so I would need a few insurance details from you. But before that, Could you please confirm your gender and date of birth for me?\"\n",
    "\n",
    "\"CUSTOMER\" : \"Sure. I'm female and born on November 19th. 1985\"\n",
    "\n",
    "\"AGENT\" : \"Great. Could you please confirm your address for me?\"\n",
    "\n",
    "\"CUSTOMER\" : \"It is 14098. Marlin coat. Mesa. Arizona.\"\n",
    "\n",
    "\"AGENT\" : \"And the Zip code B 85201.\"\n",
    "\n",
    "\"CUSTOMER\" : \"Actually, no, we are at 85296.\"\n",
    "\n",
    "\"AGENT\" : \"Okay, Great. Uh, Is this phone number 402818969 the correct number.\"\n",
    "\n",
    "\"CUSTOMER\" : \"Yes, This is the best phone number.\"\n",
    "\n",
    "\"AGENT\" : \"Okay. Could you please confirm your email ID for me?\"\n",
    "\n",
    "\"CUSTOMER\" : \"My email is m I A g 23 at gmail dot com.\"\n",
    "\n",
    "\"AGENT\" : \"Great. Uh, yeah. Could you please help me with your insurance provider and, uh, your preferred pharmacy?\"\n",
    "\n",
    "\"CUSTOMER\" : \"My insurance is with United Health. And my preferred pharmacy would be CVS.\"\n",
    "\n",
    "\"AGENT\" : \"Grade. Thank you. Let me do a final check.\"\n",
    "\n",
    "\"AGENT\" : \"Mm hmm. Great. Okay. Looks like I have all the information that is needed.\"\n",
    "\n",
    "\"AGENT\" : \"I am routing a consent form your way, how would you prefer to receive this form? text message or email?\"\n",
    "\n",
    "\"AGENT\" : \"Once you fill in the consent form and submit, we can process the further processes\"\n",
    "\n",
    "\"CUSTOMER\" : \"sure, text message is fine.\"\n",
    "\n",
    "\"AGENT\" : \"And here it goes, would you like to finish this process now when we are on the call?\"\n",
    "\n",
    "\"CUSTOMER\" : \"yes, definitely, give me few mins here\"\n",
    "\n",
    "\"AGENT\" : \"sure, take your time. I am here if you have any queries\"\n",
    "\n",
    "\"CUSTOMER\" : \"Ah, I just submitted the form\"\n",
    "\n",
    "\"AGENT\" : \"thank you, just allow me a couple of mins for it to populate it in the system\"\n",
    "\n",
    "\"AGENT\" : \"and here we go! Got evrything we need , so your enrollment process is complete\"\n",
    "\n",
    "\"CUSTOMER\" : \"that's great\"\n",
    "\n",
    "\"AGENT\" : \"Thank you, our team is going to reach out over an email in the next 48 hours\"\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "# Generate summary prompt\n",
    "#prompt_summary = get_summary_prompt(prompt_bucket, file_key, 'text_summarization', 'summary_llama2', final_transcript)\n",
    "prompt_summary = summarisation_prompt_generator(data)\n",
    "# Load Llama2 model\n",
    "model_invoke_timestamp = datetime.now()\n",
    "summary_data = load_llama2(bedrock_runtime, prompt_summary, 0.5, 0.9)\n",
    "model_invoke_after_timestamp = datetime.now()\n",
    "print(f'summary_data : lambda_handler--output_from_the_llm_model : {summary_data}.')\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "31f64696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary_data : lambda_handler--output_from_the_llm_model : \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import base64\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import logging\n",
    "import redis\n",
    "import os\n",
    "from datetime import datetime\n",
    "from botocore.config import Config\n",
    "from botocore.exceptions import ClientError\n",
    "config = Config(read_timeout=1000)\n",
    "\n",
    "# Bedrock Runtime client initialization\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name=\"bedrock-runtime\",\n",
    "    region_name= os.environ.get(\"Region\"), #\"us-east-1\",\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Bedrock client initialization\n",
    "bedrock = boto3.client(\n",
    "    service_name='bedrock',\n",
    "    region_name= os.environ.get(\"Region\") #'us-east-1'\n",
    ")\n",
    "# Initialize Redis client\n",
    "redis_host = 'ch-agent-assist-redis-cluster-disabled.tjyhst.ng.0001.use1.cache.amazonaws.com'\n",
    "redis_port = 6379  # Default Redis port is 6379\n",
    "redis_password = None  # None if no password\n",
    "redis_client = redis.Redis(host=redis_host, port=redis_port, password=redis_password, decode_responses=True)\n",
    "\n",
    "##Processing data to fetch role and content\n",
    "def construct_call_conversation(data):\n",
    "    segments = json.loads(data)\n",
    "    convo = \"\" \n",
    "    # Extract transcripts, participant roles, and content\n",
    "    for segment in segments:\n",
    "        transcript = segment['transcript'][0]\n",
    "        participant_role = transcript['ParticipantRole']\n",
    "        content = transcript['Content']\n",
    "        convo += participant_role + \" : \" + content + \"\\n\"\n",
    "        \n",
    "    return convo\n",
    "\n",
    "def model_output_postprocessing(data):\n",
    "    result = \"\"\n",
    "    start_index = data.find(\"{\")\n",
    "    \n",
    "    if start_index == -1:\n",
    "        data1 = \"{\" + data\n",
    "    else:\n",
    "        data1 = data\n",
    "    \n",
    "    start_index_final = data.find(\"{\")    \n",
    "    print(f'start_index:{start_index_final}')\n",
    "    \n",
    "    end_char_indices = [i.start() for i in re.finditer(\"}\",data1)]\n",
    "    print(f'end_indices:{end_char_indices}')\n",
    "    end_index = end_char_indices[len(end_char_indices)-1]\n",
    "    print(f'end_index:{end_index}')\n",
    "    if end_index == len(data1)-1:\n",
    "        result = data1[start_index_final:]\n",
    "    else:\n",
    "        result = data1[start_index_final:end_index+1]\n",
    "    print(f'result:{result}')\n",
    "    \n",
    "    return result\n",
    "    \n",
    "def summarisation_prompt_generator(context):\n",
    "    prompt_llama = f\"\"\"\n",
    "Instruction: \"Create a content to be sent as an email to be sent to the patient post the call, which contains any follow-up tasks or requests with spoken actions/documents \n",
    "\n",
    "\n",
    "{context}\n",
    "             \n",
    "             \n",
    "Response :\n",
    "\n",
    "    \"\"\"        \n",
    "    return prompt_llama\n",
    "\n",
    "def get_summary_prompt(bucket,file,prompt_category,required_prompt,conversation):\n",
    "    s3 = boto3.client('s3') \n",
    "    response = s3.get_object(Bucket=bucket,Key=file)\n",
    "    content = response['Body'].read().decode('utf-8')\n",
    "    json_content = json.loads(content)\n",
    "    prompt = json_content[prompt_category][required_prompt].format(context=conversation)\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "#Defining function to summarize context\n",
    "def load_llama2(bedrock_runtime , prompt , temp , top_p):\n",
    "    try:\n",
    "        body = {\n",
    "            \"prompt\" : prompt,\n",
    "            \"temperature\" : temp,\n",
    "            \"top_p\" : top_p,\n",
    "            \"max_gen_len\" : 1000\n",
    "            }\n",
    "\n",
    "        response = bedrock_runtime.invoke_model(\n",
    "            modelId=\"meta.llama2-13b-chat-v1\", body=json.dumps(body)\n",
    "        )\n",
    "\n",
    "        response_body = json.loads(response[\"body\"].read())\n",
    "        completion = response_body[\"generation\"]\n",
    "\n",
    "        return completion\n",
    "\n",
    "    except ClientError:\n",
    "        logging.error(\"Couldn't invoke Llama 2\")\n",
    "        raise  \n",
    "\n",
    "data ='''\n",
    "\"AGENT\" : \"Welcome to patient bad with this is there on March? Who do I have the pleasure of speaking with today?\"\n",
    "\n",
    "\"CUSTOMER\" : \"Hi. This is Michael Gonzalez.\"\n",
    "\n",
    "\"AGENT\" : \"Same here. How can I help you today?\"\n",
    "\n",
    "\"CUSTOMER\" : \"I wanted to inquire about enrolling in the patient service program.\"\n",
    "\n",
    "\"AGENT\" : \"Great. Uh, so I would need a few insurance details from you. But before that, Could you please confirm your gender and date of birth for me?\"\n",
    "\n",
    "\"CUSTOMER\" : \"Sure. I'm female and born on November 19th. 1985\"\n",
    "\n",
    "\"AGENT\" : \"Great. Could you please confirm your address for me?\"\n",
    "\n",
    "\"CUSTOMER\" : \"It is 14098. Marlin coat. Mesa. Arizona.\"\n",
    "\n",
    "\"AGENT\" : \"And the Zip code B 85201.\"\n",
    "\n",
    "\"CUSTOMER\" : \"Actually, no, we are at 85296.\"\n",
    "\n",
    "\"AGENT\" : \"Okay, Great. Uh, Is this phone number 402818969 the correct number.\"\n",
    "\n",
    "\"CUSTOMER\" : \"Yes, This is the best phone number.\"\n",
    "\n",
    "\"AGENT\" : \"Okay. Could you please confirm your email ID for me?\"\n",
    "\n",
    "\"CUSTOMER\" : \"My email is m I A g 23 at gmail dot com.\"\n",
    "\n",
    "\"AGENT\" : \"Great. Uh, yeah. Could you please help me with your insurance provider and, uh, your preferred pharmacy?\"\n",
    "\n",
    "\"CUSTOMER\" : \"My insurance is with United Health. And my preferred pharmacy would be CVS.\"\n",
    "\n",
    "\"AGENT\" : \"Grade. Thank you. Let me do a final check.\"\n",
    "\n",
    "\"AGENT\" : \"Mm hmm. Great. Okay. Looks like I have all the information that is needed.\"\n",
    "\n",
    "\"AGENT\" : \"I am routing a consent form your way, how would you prefer to receive this form? text message or email?\"\n",
    "\n",
    "\"AGENT\" : \"Once you fill in the consent form and submit, we can process the further processes\"\n",
    "\n",
    "\"CUSTOMER\" : \"sure, text message is fine.\"\n",
    "\n",
    "\"AGENT\" : \"And here it goes, would you like to finish this process now when we are on the call?\"\n",
    "\n",
    "\"CUSTOMER\" : \"yes, definitely, give me few mins here\"\n",
    "\n",
    "\"AGENT\" : \"sure, take your time. I am here if you have any queries\"\n",
    "\n",
    "\"CUSTOMER\" : \"Ah, I just submitted the form\"\n",
    "\n",
    "\"AGENT\" : \"thank you, just allow me a couple of mins for it to populate it in the system\"\n",
    "\n",
    "\"AGENT\" : \"and here we go! Got evrything we need , so your enrollment process is complete\"\n",
    "\n",
    "\"CUSTOMER\" : \"that's great\"\n",
    "\n",
    "\"AGENT\" : \"Thank you, our team is going to reach out over an email in the next 48 hours\"\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "# Generate summary prompt\n",
    "#prompt_summary = get_summary_prompt(prompt_bucket, file_key, 'text_summarization', 'summary_llama2', final_transcript)\n",
    "prompt_summary = summarisation_prompt_generator(data)\n",
    "# Load Llama2 model\n",
    "model_invoke_timestamp = datetime.now()\n",
    "summary_data = load_llama2(bedrock_runtime, prompt_summary, 0.5, 0.9)\n",
    "model_invoke_after_timestamp = datetime.now()\n",
    "print(f'summary_data : lambda_handler--output_from_the_llm_model : {summary_data}.')\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20a4216",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

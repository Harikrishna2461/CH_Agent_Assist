{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b09cb929",
   "metadata": {},
   "source": [
    "# Lambda : 2 : Conversational Session without History and SOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a55a6646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Based on the conversation summary, the customer's name is Jordan Smith.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import base64\n",
    "import time\n",
    "import tzlocal\n",
    "import re\n",
    "import pandas as pd\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name=\"bedrock-runtime\",\n",
    "    region_name=\"us-east-1\",\n",
    ")\n",
    "\n",
    "bedrock = boto3.client(\n",
    "    service_name='bedrock', \n",
    "    region_name='us-east-1'\n",
    ")\n",
    "\n",
    "##Put data here\n",
    "##Loading JSON to read data\n",
    "file_path = \"Conversation_2_14_formatted.json\"\n",
    "# Open the JSON file for reading\n",
    "with open(file_path, 'r') as file:\n",
    "    # Parse the JSON file\n",
    "    data = json.load(file)\n",
    "\n",
    "##Processing data to fetch role and content\n",
    "def data_preprocessing(transcription):\n",
    "    convo = \"\"\n",
    "    for i in range(len(transcription['transcriptions'])):\n",
    "        convo = convo + transcription['transcriptions'][i]['ParticipantRole'] + \": \" + transcription['transcriptions'][i]['Content']\n",
    "        convo += \"\\n\"\n",
    "    return convo\n",
    "\n",
    "def data_postprocessing(data):\n",
    "    result = \"\"\n",
    "    start_index = data.find(\"{\")\n",
    "    end_char_indices = [i.start() for i in re.finditer(\"}\",data)]\n",
    "    end_index = end_char_indices[len(end_char_indices)-1]\n",
    "    result = data[start_index:end_index+1]\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_data_in_text(data):\n",
    "    text_data = \"\"\n",
    "    keys = list(data.keys())\n",
    "    for key in keys:\n",
    "        value = data[key]\n",
    "        text_data += f\"The {key} is {value}\"\n",
    "    return text_data\n",
    "\n",
    "final_transcript = data_preprocessing(data)\n",
    "##Change/Update the preprocessing logic\n",
    "\n",
    "#Defining function to connect to Bedrock LLM\n",
    "def load_claude2(bedrock_runtime , prompt , temp , top_p,top_k):\n",
    "    try:\n",
    "        body = {\n",
    "            \"prompt\": prompt,\n",
    "            \"temperature\": temp,\n",
    "            \"top_p\": top_p,\n",
    "            \"top_k\":top_k,\n",
    "            \"max_tokens_to_sample\": 1000\n",
    "            }\n",
    "\n",
    "        response = bedrock_runtime.invoke_model(\n",
    "            modelId=\"anthropic.claude-v2\", body=json.dumps(body), accept=\"application/json\", contentType=\"application/json\"\n",
    "                 )\n",
    "        \n",
    "        response_body = json.loads(response[\"body\"].read())\n",
    "        completion = response_body.get(\"completion\")\n",
    "\n",
    "        return completion\n",
    "\n",
    "    except ClientError:\n",
    "        logging.error(\"Couldn't invoke Claude 2\")\n",
    "        raise\n",
    "\n",
    "#Defining function to summarize context\n",
    "def load_text_summariser_llama2(bedrock_runtime , prompt , temp , top_p):\n",
    "    try:\n",
    "        body = {\n",
    "            \"prompt\" : prompt,\n",
    "            \"temperature\" : temp,\n",
    "            \"top_p\" : top_p,\n",
    "            \"max_gen_len\" : 200\n",
    "            }\n",
    "\n",
    "        response = bedrock_runtime.invoke_model(\n",
    "            modelId=\"meta.llama2-13b-chat-v1\", body=json.dumps(body)\n",
    "        )\n",
    "\n",
    "        response_body = json.loads(response[\"body\"].read())\n",
    "        completion = response_body[\"generation\"]\n",
    "\n",
    "        return completion\n",
    "\n",
    "    except ClientError:\n",
    "        logger.error(\"Couldn't invoke Llama 2\")\n",
    "        raise\n",
    "        \n",
    "#Defining function to summarize context\n",
    "def load_llama2(bedrock_runtime , prompt , temp , top_p):\n",
    "    try:\n",
    "        body = {\n",
    "            \"prompt\" : prompt,\n",
    "            \"temperature\" : temp,\n",
    "            \"top_p\" : top_p,\n",
    "            \"max_gen_len\" : 1000\n",
    "            }\n",
    "\n",
    "        response = bedrock_runtime.invoke_model(\n",
    "            modelId=\"meta.llama2-13b-chat-v1\", body=json.dumps(body)\n",
    "        )\n",
    "\n",
    "        response_body = json.loads(response[\"body\"].read())\n",
    "        completion = response_body[\"generation\"]\n",
    "\n",
    "        return completion\n",
    "\n",
    "    except ClientError:\n",
    "        logger.error(\"Couldn't invoke Llama 2\")\n",
    "        raise\n",
    "        \n",
    "#Defining LLM function for the prompt generator for entity extraction  \n",
    "def enrollment_prompt_generator(conversation,entities):\n",
    "    prompt_claude = \"\"\"Human: \\\"\"\"\" + conversation + \"\"\"\\\"\n",
    "\n",
    "    The above is a transcript between a call center agent and an insurance subscriber or patient. Identify and extract key entities such as \\\"\"\"\" + entities + \"\"\"\\\" from the transcript. Include only the information present.\n",
    "\n",
    "    Output the results as a structured JSON containing only the extracted fields.\n",
    "\n",
    "    Assistant:\n",
    "    \"\"\"\n",
    "\n",
    "    return prompt_claude\n",
    "\n",
    "def summarisation_prompt_generator(context):\n",
    "    prompt_llama = f\"\"\"\n",
    "Instruction: \"Summarise this call transcript between a patient and an agent and provide it in a precise paragraph : \" :\n",
    "\n",
    "{context}.\n",
    "             \n",
    "Response :  \n",
    "    \"\"\"\n",
    "    return prompt_llama\n",
    "\n",
    "def QnA_prompt_generator(final_summary,patient_data,question):\n",
    "    prompt_llama = f\"\"\"Using the information from the conversation summary : {final_summary} \n",
    "and \n",
    "The patient enrollment data : \n",
    "{patient_data},\n",
    "Answer the following Question : \n",
    "{question}\n",
    "\n",
    "If you do not know the answer and if the Current conversation summary or the Patient enrollment data doesn't contain the answer,then \n",
    "truthfully say I don't know.\n",
    "Response:\n",
    "\"\"\"\n",
    "    return prompt_llama\n",
    "\n",
    "def QnA_prompt_generator_claude2(final_summary,patient_data,question):\n",
    "    prompt_llama = f\"\"\"Human: \\\"\"\"\" + question + \"\"\"\\\"\n",
    "\n",
    "The above is a question/query asked by a patient to a call center agent.Using the conversation_summary which is \\\"\"\"\" + final_summary + \"\"\"\\\" \n",
    "and the patient's enrollment data which is \\\"\"\"\" + patient_data + \"\"\"\\\",answer the patient's question or query.\n",
    "\n",
    "Assistant:\n",
    "\"\"\"\n",
    "    return prompt_llama\n",
    "\n",
    "# Lambda handler\n",
    "def get_enrollment_data(transcript):\n",
    "    entities = \"name of patient, status of insurance, insurance number, demographic details\"\n",
    "    prompt_enrollment = enrollment_prompt_generator(transcript,entities)\n",
    "    enrollment_data = load_claude2(bedrock_runtime,prompt_enrollment,0,0.9,1)\n",
    "    json_data = data_postprocessing(enrollment_data)\n",
    "    enrollment_json_object = json.loads(json_data)\n",
    "    return enrollment_json_object\n",
    "\n",
    "def get_answer(data,question):\n",
    "    transcript = data_preprocessing(data)\n",
    "    patient_data = get_data_in_text(get_enrollment_data(transcript))\n",
    "    summary_prompt = summarisation_prompt_generator(transcript)\n",
    "    final_summary = load_text_summariser_llama2(bedrock_runtime,summary_prompt,0.5,0.9)\n",
    "    QnA_prompt = QnA_prompt_generator_claude2(final_summary,patient_data,question)\n",
    "    final_answer_llama_2 = load_claude2(bedrock_runtime,QnA_prompt,0.5,0.9,1)\n",
    "    return final_answer_llama_2\n",
    "\n",
    "\n",
    "# Lambda handler to intgerate with AWS\n",
    "def lambda_handler(data,question_data):\n",
    "    question = data_preprocessing(question_data)\n",
    "    answer = get_answer(data,question)\n",
    "    #enrollment_json_object = json.loads(answer)\n",
    "    return answer#{\"statusCode\": 200,\"body\": json.dumps(answer)}\n",
    "\n",
    "question_data = {'stream': 'TRANSCRIPTION',\n",
    " 'contactId': 'aa621db9-934b-462c-bc7a-85f2e01c4c9f',\n",
    " 'transcriptions': [{'ParticipantId': 'CUSTOMER',\n",
    "   'ParticipantRole': 'CUSTOMER',\n",
    "   'Content': \"What is the customer's name?\",\n",
    "   'BeginOffsetMillis': 1257,\n",
    "   'EndOffsetMillis': 9697,\n",
    "   'Id': 'fb29489e-d06b-48b1-8e0b-519f17c4a68e',\n",
    "   'Sentiment': 'NEUTRAL',\n",
    "   'IssuesDetected': []}]}\n",
    "\n",
    "#while transcripts != None:\n",
    "lambda_handler(data,question_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
